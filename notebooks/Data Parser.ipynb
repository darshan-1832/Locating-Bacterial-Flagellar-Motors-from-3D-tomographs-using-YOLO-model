{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c56478-3783-4a0a-b490-a4b8d8c44caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7349f0a8-91fe-4a71-bf4f-f5f1989440e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import time\n",
    "import yaml\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46920d8d-5afa-4890-97b0-a037ece6589d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of motors in the dataset: 831\n",
      "Found 362 unique tomograms with motors\n",
      "Split: 289 tomograms for training, 73 tomograms for validation\n",
      "Will process approximately 3267 slices for training\n",
      "Will process approximately 792 slices for validation\n",
      "\n",
      "Processing Summary:\n",
      "- Train set: 289 tomograms, 363 motors, 3262 slices\n",
      "- Validation set: 73 tomograms, 88 motors, 792 slices\n",
      "- Total: 362 tomograms, 451 motors, 4054 slices\n",
      "\n",
      "Preprocessing Complete:\n",
      "- Training data: 289 tomograms, 363 motors, 3262 slices\n",
      "- Validation data: 73 tomograms, 88 motors, 792 slices\n",
      "- Dataset directory: C:\\Users\\puvia\\OneDrive\\Documents\\GitHub\\Locating-Bacterial-Flagellar-Motors-from-3D-tomographs-using-YOLO-model\\data\\yolo_dataset\n",
      "- YAML configuration: C:\\Users\\puvia\\OneDrive\\Documents\\GitHub\\Locating-Bacterial-Flagellar-Motors-from-3D-tomographs-using-YOLO-model\\data\\yolo_dataset\\dataset.yaml\n",
      "\n",
      "Ready for YOLO training!\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define dataset paths\n",
    "data_path = r\"C:\\Users\\puvia\\OneDrive\\Documents\\GitHub\\Locating-Bacterial-Flagellar-Motors-from-3D-tomographs-using-YOLO-model\\data\"\n",
    "train_dir = os.path.join(data_path, \"train\")\n",
    "\n",
    "# Define YOLO dataset structure\n",
    "yolo_dataset_dir = r\"C:\\Users\\puvia\\OneDrive\\Documents\\GitHub\\Locating-Bacterial-Flagellar-Motors-from-3D-tomographs-using-YOLO-model\\data\\yolo_dataset\"\n",
    "yolo_images_train = os.path.join(yolo_dataset_dir, \"images\", \"train\")\n",
    "yolo_images_val = os.path.join(yolo_dataset_dir, \"images\", \"val\")\n",
    "yolo_labels_train = os.path.join(yolo_dataset_dir, \"labels\", \"train\")\n",
    "yolo_labels_val = os.path.join(yolo_dataset_dir, \"labels\", \"val\")\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [yolo_images_train, yolo_images_val, yolo_labels_train, yolo_labels_val]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Constants\n",
    "TRUST = 4  # Number of slices above and below center slice (total 2*TRUST + 1 slices)\n",
    "BOX_SIZE = 24  # Bounding box size for annotations (in pixels)\n",
    "TRAIN_SPLIT = 0.8  # 80% for training, 20% for validation\n",
    "\n",
    "def normalize_slice(slice_data):\n",
    "    \"\"\"\n",
    "    We convert the images into arrays and use only the pixels between 2 to 98 percentiles which means, not too dark or white.\n",
    "    \"\"\"\n",
    "    p2, p98 = np.percentile(slice_data, [2, 98])\n",
    "    clipped_data = np.clip(slice_data, p2, p98)\n",
    "    \n",
    "    # Normalize to [0, 255] range\n",
    "    normalized = 255 * (clipped_data - p2) / (p98 - p2)\n",
    "    \n",
    "    return np.uint8(normalized)\n",
    "\n",
    "def prepare_yolo_dataset(trust=TRUST, train_split=TRAIN_SPLIT):\n",
    "    \"\"\"\n",
    "    Extract slices containing motors from tomograms and save to YOLO structure with annotations\n",
    "    \"\"\"\n",
    "    # Load the labels CSV\n",
    "    labels_df = pd.read_csv(os.path.join(data_path, \"train_labels.csv\"))\n",
    "    \n",
    "    # Count total number of motors\n",
    "    total_motors = labels_df['Number of motors'].sum()\n",
    "    print(f\"Total number of motors in the dataset: {total_motors}\")\n",
    "    \n",
    "    # Get unique tomograms that have motors\n",
    "    tomo_df = labels_df[labels_df['Number of motors'] > 0].copy()\n",
    "    unique_tomos = tomo_df['tomo_id'].unique()\n",
    "    \n",
    "    print(f\"Found {len(unique_tomos)} unique tomograms with motors\")\n",
    "    \n",
    "    # Perform the train-val split at the tomogram level (not motor level)\n",
    "    # This ensures all slices from a single tomogram go to either train or val\n",
    "    np.random.shuffle(unique_tomos)  # Shuffle the tomograms\n",
    "    split_idx = int(len(unique_tomos) * train_split)\n",
    "    train_tomos = unique_tomos[:split_idx]\n",
    "    val_tomos = unique_tomos[split_idx:]\n",
    "    \n",
    "    print(f\"Split: {len(train_tomos)} tomograms for training, {len(val_tomos)} tomograms for validation\")\n",
    "    \n",
    "    # Function to process a set of tomograms\n",
    "    def process_tomogram_set(tomogram_ids, images_dir, labels_dir, set_name):\n",
    "        motor_counts = []\n",
    "        for tomo_id in tomogram_ids:\n",
    "            # Get all motors for this tomogram\n",
    "            tomo_motors = labels_df[labels_df['tomo_id'] == tomo_id]\n",
    "            for _, motor in tomo_motors.iterrows():\n",
    "                if pd.isna(motor['Motor axis 0']):\n",
    "                    continue\n",
    "                motor_counts.append(\n",
    "                    (tomo_id, \n",
    "                     int(motor['Motor axis 0']), \n",
    "                     int(motor['Motor axis 1']), \n",
    "                     int(motor['Motor axis 2']),\n",
    "                     int(motor['Array shape (axis 0)']))\n",
    "                )\n",
    "        \n",
    "        print(f\"Will process approximately {len(motor_counts) * (2 * trust + 1)} slices for {set_name}\")\n",
    "        \n",
    "        # Process each motor\n",
    "        processed_slices = 0\n",
    "        \n",
    "        for tomo_id, z_center, y_center, x_center, z_max in motor_counts:\n",
    "            # Calculate range of slices to include\n",
    "            z_min = max(0, z_center - trust)\n",
    "            z_max = min(z_max - 1, z_center + trust)\n",
    "            \n",
    "            # Process each slice in the range\n",
    "            for z in range(z_min, z_max + 1):\n",
    "                # Create slice filename\n",
    "                slice_filename = f\"slice_{z:04d}.jpg\"\n",
    "                \n",
    "                # Source path for the slice\n",
    "                src_path = os.path.join(train_dir, tomo_id, slice_filename)\n",
    "                \n",
    "                if not os.path.exists(src_path):\n",
    "                    print(f\"Warning: {src_path} does not exist, skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                # Load and normalize the slice\n",
    "                img = Image.open(src_path)\n",
    "                img_array = np.array(img)\n",
    "                \n",
    "                # Normalize the image\n",
    "                normalized_img = normalize_slice(img_array)\n",
    "                \n",
    "                # Create destination filename (with unique identifier)\n",
    "                dest_filename = f\"{tomo_id}_z{z:04d}_y{y_center:04d}_x{x_center:04d}.jpg\"\n",
    "                dest_path = os.path.join(images_dir, dest_filename)\n",
    "                \n",
    "                # Save the normalized image\n",
    "                Image.fromarray(normalized_img).save(dest_path)\n",
    "                \n",
    "                # Get image dimensions\n",
    "                img_width, img_height = img.size\n",
    "                \n",
    "                # Create YOLO format label\n",
    "                # YOLO format: <class> <x_center> <y_center> <width> <height>\n",
    "                # Values are normalized to [0, 1]\n",
    "                x_center_norm = x_center / img_width\n",
    "                y_center_norm = y_center / img_height\n",
    "                box_width_norm = BOX_SIZE / img_width\n",
    "                box_height_norm = BOX_SIZE / img_height\n",
    "                \n",
    "                # Write label file\n",
    "                label_path = os.path.join(labels_dir, dest_filename.replace('.jpg', '.txt'))\n",
    "                with open(label_path, 'w') as f:\n",
    "                    f.write(f\"0 {x_center_norm} {y_center_norm} {box_width_norm} {box_height_norm}\\n\")\n",
    "                \n",
    "                processed_slices += 1\n",
    "        \n",
    "        return processed_slices, len(motor_counts)\n",
    "    \n",
    "    # Process training tomograms\n",
    "    train_slices, train_motors = process_tomogram_set(train_tomos, yolo_images_train, yolo_labels_train, \"training\")\n",
    "    \n",
    "    # Process validation tomograms\n",
    "    val_slices, val_motors = process_tomogram_set(val_tomos, yolo_images_val, yolo_labels_val, \"validation\")\n",
    "    \n",
    "    # Create YAML configuration file for YOLO\n",
    "    yaml_content = {\n",
    "        'path': yolo_dataset_dir,\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'names': {0: 'motor'}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(yolo_dataset_dir, 'dataset.yaml'), 'w') as f:\n",
    "        yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"- Train set: {len(train_tomos)} tomograms, {train_motors} motors, {train_slices} slices\")\n",
    "    print(f\"- Validation set: {len(val_tomos)} tomograms, {val_motors} motors, {val_slices} slices\")\n",
    "    print(f\"- Total: {len(train_tomos) + len(val_tomos)} tomograms, {train_motors + val_motors} motors, {train_slices + val_slices} slices\")\n",
    "    \n",
    "    # Return summary info\n",
    "    return {\n",
    "        \"dataset_dir\": yolo_dataset_dir,\n",
    "        \"yaml_path\": os.path.join(yolo_dataset_dir, 'dataset.yaml'),\n",
    "        \"train_tomograms\": len(train_tomos),\n",
    "        \"val_tomograms\": len(val_tomos),\n",
    "        \"train_motors\": train_motors,\n",
    "        \"val_motors\": val_motors,\n",
    "        \"train_slices\": train_slices,\n",
    "        \"val_slices\": val_slices\n",
    "    }\n",
    "\n",
    "# Run the preprocessing\n",
    "summary = prepare_yolo_dataset(TRUST)\n",
    "print(f\"\\nPreprocessing Complete:\")\n",
    "print(f\"- Training data: {summary['train_tomograms']} tomograms, {summary['train_motors']} motors, {summary['train_slices']} slices\")\n",
    "print(f\"- Validation data: {summary['val_tomograms']} tomograms, {summary['val_motors']} motors, {summary['val_slices']} slices\")\n",
    "print(f\"- Dataset directory: {summary['dataset_dir']}\")\n",
    "print(f\"- YAML configuration: {summary['yaml_path']}\")\n",
    "print(f\"\\nReady for YOLO training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
